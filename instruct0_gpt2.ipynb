{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeVCidVlpnYf"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class InstructGPT:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = 'gpt2-medium',\n",
        "        max_length: int = 420\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        # Change this to a better dataset because this one sucks\n",
        "        dataset = load_dataset(\"Dahoas/instruct-synthetic-prompt-responses\")\n",
        "\n",
        "        # Combine prompts\n",
        "        def combine_prompt_response(example):\n",
        "            full_text = f\"{example['prompt']} {self.tokenizer.eos_token} {example['response']}\"\n",
        "            return {'text': full_text}\n",
        "        dataset = dataset.map(combine_prompt_response, remove_columns=dataset['train'].column_names)\n",
        "        return dataset\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "    def prepare_training_data(self, test_size: float = 0.1):\n",
        "\n",
        "        tokenized_datasets = self.dataset.map(\n",
        "            self.tokenize_function,\n",
        "            remove_columns=self.dataset['train'].column_names\n",
        "\n",
        "            batched=True,\n",
        "        )\n",
        "\n",
        "        # Split into train and validation\n",
        "        tokenized_datasets = tokenized_datasets['train'].train_test_split(test_size=test_size)\n",
        "\n",
        "        return tokenized_datasets\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        output_dir: str = './instruct-gpt2-model',\n",
        "        learning_rate: float = 5e-5,\n",
        "        batch_size: int = 5,\n",
        "        num_train_epochs: int = 5, # not really sure about this stuff yet, may want to tweak this later\n",
        "    ):\n",
        "\n",
        "        self.dataset = self.prepare_dataset() # Call prepare_dataset to initialize self.dataset\n",
        "        tokenized_datasets = self.prepare_training_data()\n",
        "\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            eval_steps=100,\n",
        "            prediction_loss_only=True,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_datasets['train'],\n",
        "            eval_dataset=tokenized_datasets['test'],\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "\n",
        "        # Stuff from Claude because I apparently messed up a bunch of things\n",
        "    def generate_text(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 420,\n",
        "        temperature: float = 0.5,\n",
        "        top_p: float = 0.9\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate text using trained model\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Input prompt\n",
        "            max_length (int): Maximum generation length\n",
        "            temperature (float): Sampling temperature\n",
        "            top_p (float): Nucleus sampling threshold\n",
        "\n",
        "        Returns:\n",
        "            Generated text\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "        output = self.model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def main():\n",
        "    instruct_trainer = InstructGPT()\n",
        "    instruct_trainer.train(\n",
        "        output_dir='./instruct-gpt2-model',\n",
        "        learning_rate=5e-5,\n",
        "        batch_size=5,\n",
        "        num_train_epochs=5\n",
        "    )\n",
        "\n",
        "    prompt = \"Tell me a story about Poland\"\n",
        "    generated_text = instruct_trainer.generate_text(prompt)\n",
        "    print(\"Response::\", generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GmOwUzC8p0Cr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}